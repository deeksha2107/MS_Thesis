{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cdd653f-72fc-4c70-93fe-694b48e1a3d7",
   "metadata": {},
   "source": [
    "Dataset Collection In this code, we obtain the dataset from three sources, the Enron dataset, Millersmile, and Nazario dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f9d36-4b48-4ab2-90cc-f6ed28a8c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation of allthe required libraries\n",
    "!pip install requests\n",
    "!pip install re\n",
    "!pip install langdetect\n",
    "!pip install nltk\n",
    "!pip install autocorrect\n",
    "!pip install googletrans==3.1.0a0 \n",
    "!pip install wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636c8ca-f5de-4b7c-968b-9a1b0e32f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary libraries \n",
    "import langdetect\n",
    "import wordninja\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import email\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from autocorrect import Speller\n",
    "from email import message_from_string\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from googletrans import Translator\n",
    "from email import message_from_string\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5459410-5a54-484a-9a50-1fc08036e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the stopwords in English language\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4933fd-e415-4ea8-9f7e-f7caed5f1e5e",
   "metadata": {},
   "source": [
    "Millersmile Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155b6aa-c557-4551-8fe6-5e44613131d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe phish_emails_df created with columns - Name ,Subject and Text\n",
    "phish_emails_df = pd.DataFrame(columns = ['Name', 'Subject', 'Text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9790a0-9b55-486e-b8ed-35368de880b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web scraping\n",
    "spam_email_data = []\n",
    "count_spam_emails = 0\n",
    "count_ham_emails = 51000\n",
    "for i in range(296, 335 , 1):\n",
    "    url = \"http://www.millersmiles.co.uk/archives/\"+str(i)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    #Finding all occurrences with orange font and skipping the first one\n",
    "    orange_fonts = soup.find_all('font',{'color':'orange'})[1:]\n",
    "    for orange_font in orange_fonts:\n",
    "        #Second orange_font is the name\n",
    "        name = orange_font.get_text(strip=True)\n",
    "        #Subject is a hyperlink\n",
    "        subject = orange_font.find_next('a').get_text(strip=True)\n",
    "        # Text is enclosed within blockquote\n",
    "        text = orange_font.find_next('blockquote').get_text(strip=True)\n",
    "        spam_email_data.append({'Name':name, 'Subject':subject, 'Text': text})\n",
    "        count_spam_emails+=1\n",
    "        if count_spam_emails >= count_ham_emails:\n",
    "            break\n",
    "    if count_spam_emails >= count_ham_emails:\n",
    "            break\n",
    "phish_emails_df = pd.DataFrame(spam_email_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bfa7e-1cd2-4465-a534-e571277dc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation of subject and content into a new column text\n",
    "phish_emails_df['text'] = phish_emails_df['Subject']+phish_emails_df['Text']\n",
    "phish_emails_df['label'] = 1\n",
    "phish_emails_df = phish_emails_df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411f9b6-0a80-4ba3-b29c-137a21c6e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In dataset 2, we use a total of 2927 emails with training data length being 2342 and testing data length being around 585.\n",
    "no_emails = 2927\n",
    "train_data_len = 2342\n",
    "phish_emails_df = phish_emails_df.sample(n = no_emails)\n",
    "org_phish_train = phish_emails_df.sample(n = train_data_len)\n",
    "org_phish_test = phish_emails_df.drop(org_phish_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fad232-fcfc-4567-b65f-3cb38630a32b",
   "metadata": {},
   "source": [
    "Nazario Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0b8d3-d42f-4b90-8632-efd325ce6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Nazario dataset\n",
    "emails_naz = pd.read_csv(\"Nazario.csv\")\n",
    "emails_naz['text'] = emails_naz['subject']+emails_naz['body']\n",
    "emails_naz['label'] = 1\n",
    "emails_naz_df = emails_naz[['text', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa29e6-95bc-4785-861b-ae584b1d0c8c",
   "metadata": {},
   "source": [
    "Enron Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd8fc2-5337-4040-8ece-3ef58bf7be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emails.csv file is the Enron dataset stored in Kaggle\n",
    "emails = pd.read_csv(\"emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30adbc4d-2b79-4e85-8e36-15096cfffc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_date(date_string):\n",
    "    if not date_string:\n",
    "        return None\n",
    "    try:\n",
    "        return parser.parse(date_string)\n",
    "    except (ValueError, parser.ParserError):\n",
    "        return None\n",
    "\n",
    "df_email = pd.DataFrame([\n",
    "    {\n",
    "        'Content': complete_content.get_payload(),\n",
    "        'Subject': d.get('Subject', 'No Subject'),\n",
    "        'Date': d.get('Date'),  # Store the original date string\n",
    "        'Label': 0\n",
    "    }\n",
    "    for i in range(emails.shape[0])\n",
    "    for email in [emails.loc[i]['message']]\n",
    "    for complete_content in [message_from_string(email)]\n",
    "    for d in [dict(complete_content.items())]\n",
    "])\n",
    "\n",
    "# Convert 'Date' column to datetime\n",
    "df_email['Date'] = pd.to_datetime(df_email['Date'].apply(safe_parse_date), errors='coerce')\n",
    "\n",
    "# Extract year from the Date column\n",
    "df_email['Year'] = df_email['Date'].dt.year\n",
    "\n",
    "# Concatenation of subject and content into a new column text\n",
    "df_email['text'] = df_email['Subject'] + df_email['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9b393-ed5a-49db-8f19-80a97dd6fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for emails from August 2001 onwards till December 2002\n",
    "df_email['Date'] = pd.to_datetime(df_email['Date'])\n",
    "start_date = pd.Timestamp('2001-08-01', tz='UTC')  # Or use the correct timezone\n",
    "end_date = pd.Timestamp('2002-12-31 23:59:59', tz='UTC')\n",
    "\n",
    "filtered_emails = df_email[(df_email['Date'] >= start_date) & (df_email['Date'] <= end_date)]\n",
    "filtered_emails[\"label\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a932cb-fa4b-4b6a-a939-adfc34508865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Among a total of 22458 emails, 17967 emails are used for training and 4491 for testing \n",
    "no_of_emails = 22458\n",
    "df_email = filtered_emails.sample(n=no_of_emails)\n",
    "phish_emails_df_final = df_email[['text', 'label']]\n",
    "num_of_train_emails = 17967\n",
    "leg_train_df = phish_emails_df_final.head(num_of_train_emails)\n",
    "leg_test_df = phish_emails_df_final.drop(leg_train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae0da5-8810-470b-8824-ec2d0bab2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset2_Train = pd.concat([org_phish_train, naz_spam_train, leg_train_df], axis = 0)\n",
    "Dataset2_Test = pd.concat([org_phish_test, naz_spam_test, leg_test_df], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156aa0e2-479a-4148-930b-3ea3bf9b1597",
   "metadata": {},
   "source": [
    "Tradional Preprocessing - The following fuctions are involved in the preprocessing stage\n",
    "1) Removal of special characters\n",
    "2) Conversion to lowercase\n",
    "3) Removal of stop words\n",
    "4) Removal of Numbers\n",
    "5) Identification of URL, Numbers, Phone, Email\n",
    "6) Detection and translation of languages (if not in English)\n",
    "These preprocessing steps are followed to obtain the Dataset2_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12faae0e-e226-4ebe-8f55-fdf5d7dd8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove special characters from a given text string\n",
    "def remove_special_characters(text):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    cleaned_text = re.sub(r'_+', '', cleaned_text)\n",
    "    return cleaned_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3ba4f-1d65-4238-a301-91f4b8a1c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the text into lowercase\n",
    "def convert_to_lowercase(text):\n",
    "    words_list = [word.lower() for word in text.split()]\n",
    "    cleaned_text = ' '.join(words_list)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91369006-48f1-45f2-a81e-8c0f5fbc16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove stop words from text\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_list = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(words_list)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6143d-7fd0-4ccc-b914-9e651d7b1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove numbers from the text\n",
    "def remove_numbers(text):\n",
    "    words = text.split()\n",
    "    cleaned_text_list = []\n",
    "    for word in words:\n",
    "        word_list = [x for x in word if x.isdigit()!=True]\n",
    "        cleaned_text_list.append(\"\".join(word_list))\n",
    "    cleaned_text = \" \".join(cleaned_text_list)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb02ad-f70c-45bb-84d6-de4460793051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify attachment and urls in the text\n",
    "def url_attachment_identification(text):\n",
    "    url_pattern = r'\\b(?:https?:\\/\\/)?(?:[\\w-]+\\.)+[a-z]{2,}(?:\\/[^\\s]*)?'\n",
    "    attachment_pattern = r'\\b\\w+\\.(pdf|docx|jpg|png|xls|xlsx|ppt|pptx|txt|zip)\\b'\n",
    "    email_pattern = r'\\b[a-zA-Z0-9_.%+-]+@[a-zA-Z0-9.-]+\\.[a-z|A-Z]{2,7}\\b'\n",
    "    phone_pattern = r'^\\+?(\\d{1,3})?[-.\\s]?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}([-.\\s]?(ext|x|#)\\s?\\d{1,5})?$' \n",
    "\n",
    "    cleaned_text = re.sub(email_pattern,\"email\",text)\n",
    "    cleaned_text = re.sub(phone_pattern,\"phone number\",cleaned_text)\n",
    "    cleaned_text = re.sub(url_pattern,\"link\",cleaned_text)\n",
    "    cleaned_text = re.sub(attachment_pattern,\"attachment\",cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacbda0c-d97b-4cb8-97e0-f62bb2a4fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection and translation of the email \n",
    "def detect_and_translate(text):\n",
    "    try:\n",
    "        detected_lang = langdetect.detect(text)\n",
    "        if detected_lang == 'en':\n",
    "            return text\n",
    "        translator = Translator()\n",
    "        translation = translator.translate(text, dest='en')\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4387fc4-936c-4f0e-90b2-d34bb9f93374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the emails\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    cleaned_text = url_attachment_identification(text)\n",
    "    cleaned_text = remove_special_characters(cleaned_text)\n",
    "    cleaned_text = convert_to_lowercase(cleaned_text)\n",
    "    cleaned_text = remove_stop_words(cleaned_text)\n",
    "    cleaned_text = remove_numbers(cleaned_text)\n",
    "    cleaned_text = detect_and_translate(cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3f19f-98ff-4048-aa31-b36296159dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the text column of dataframe df_email\n",
    "Dataset2_Train['cleaned_text'] = Dataset2_Train[\"text\"].apply(preprocess)\n",
    "Dataset2_Test['cleaned_text'] = Dataset2_Test[\"text\"].apply(preprocess)\n",
    "Dataset2_1Train = Dataset2_Train#[[\"cleaned_text\",\"label\"]]\n",
    "Dataset2_1Test = Dataset2_Test#[[\"cleaned_text\",\"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a8e9f-5621-485b-be89-062e66eb314a",
   "metadata": {},
   "source": [
    "Traditional + Split_Words + SpellChecker\n",
    "Along with the traditional methods used, we also use the second level of preprocessing functions - \n",
    "1) Splitting of Words\n",
    "2) Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010eca7-e665-47ed-bdd2-0321a28f84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset2_1Train.to_csv(\"Dataset2_1Train_new2.csv\")\n",
    "Dataset2_1Test.to_csv(\"Dataset2_1Test_new2.csv\")\n",
    "# Replace NaN values with an empty string\n",
    "Dataset2_1Train = pd.read_csv(\"Dataset2_1Train_new2.csv\")\n",
    "Dataset2_1Train['cleaned_text'] = Dataset2_1Train['cleaned_text'].fillna('')\n",
    "Dataset2_1Train.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# Replace NaN values with an empty string\n",
    "Dataset2_1Test = pd.read_csv(\"Dataset2_1Test_new2.csv\")\n",
    "Dataset2_1Test['cleaned_text'] = Dataset2_1Test['cleaned_text'].fillna('')\n",
    "Dataset2_1Test.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17194608-d399-4ea7-8421-952269c2f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autocorrection of email\n",
    "def autocorrect_email(text):\n",
    "    check = Speller(lang=\"en\")\n",
    "    autocorrected_email = check(text)\n",
    "    return autocorrected_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd36a00-e1bc-4151-afa6-179af1f7f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing of Dataset1_1Train\n",
    "for index, row in Dataset2_1Train.iterrows():\n",
    "    email = row['cleaned_text']\n",
    "    autocorrected_email = autocorrect_email(email)\n",
    "    split_words = wordninja.split(autocorrected_email)\n",
    "    cleaned_email = \" \".join(split_words)\n",
    "    Dataset2_1Train.loc[index, 'final_cleaned_text'] = cleaned_email\n",
    "\n",
    "# Processing of Dataset1_1Test \n",
    "for index, row in Dataset2_1Test.iterrows():\n",
    "    email = row['cleaned_text']\n",
    "    autocorrected_email = autocorrect_email(email)\n",
    "    split_words = wordninja.split(autocorrected_email)\n",
    "    cleaned_email = \" \".join(split_words)\n",
    "    Dataset2_1Test.loc[index, 'final_cleaned_text'] = cleaned_email\n",
    "\n",
    "# Extract necessary columns\n",
    "Dataset2_2Train = Dataset2_1Train#[[\"final_cleaned_text\",\"label\"]]\n",
    "Dataset2_2Test = Dataset2_1Test#[[\"final_cleaned_text\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e689106-e9b4-4135-9eda-1c6eb574b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = end_time - start_time\n",
    "print(f\"Total time to create Dataset 2_2 is {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4fd1c-5f09-4da6-a8a5-bf3e1634553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of the dataframe into csv\n",
    "#Dataset2_Train.to_csv(\"Dataset2_Train.csv\")\n",
    "#Dataset2_Test.to_csv(\"Dataset2_Test.csv\")\n",
    "#Dataset2_1Train.to_csv(\"Dataset2_1Train_new2.csv\")\n",
    "#Dataset2_1Test.to_csv(\"Dataset2_1Test_new2.csv\")\n",
    "#Dataset2_2Train.to_csv(\"Dataset2_2Train_new2.csv\")\n",
    "#Dataset2_2Test.to_csv(\"Dataset2_2Test_new2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
