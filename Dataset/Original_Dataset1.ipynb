{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46aed08-912a-44f4-81d5-6d2c22b8ee31",
   "metadata": {},
   "source": [
    "Dataset Collection\n",
    "In this code, we obtain the dataset from two sources, the Enron dataset and the Millersmile dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20c5c8-1a4e-46bd-b356-3d237e0fe939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing all the necessary libraries\n",
    "!pip install requests\n",
    "!pip install re\n",
    "!pip install langdetect\n",
    "!pip install nltk\n",
    "!pip install autocorrect\n",
    "!pip install googletrans==3.1.0a0 \n",
    "!pip install wordninja\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2636c8ca-f5de-4b7c-968b-9a1b0e32f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import email\n",
    "import langdetect\n",
    "import wordninja\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser\n",
    "\n",
    "from autocorrect import Speller\n",
    "from email import message_from_string\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from googletrans import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5459410-5a54-484a-9a50-1fc08036e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the stopwords in English language\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6155b6aa-c557-4551-8fe6-5e44613131d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe phish_emails_df created with columns - Name, Subject and Text\n",
    "phish_emails_df = pd.DataFrame(columns = ['Name', 'Subject', 'Text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9790a0-9b55-486e-b8ed-35368de880b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web scraping\n",
    "spam_email_data = []\n",
    "count_spam_emails = 0\n",
    "count_ham_emails = 51000\n",
    "for i in range(1, 296, 1):\n",
    "    url = \"http://www.millersmiles.co.uk/archives/\"+str(i)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    #Finding all occurrences with orange font and skipping the first one\n",
    "    orange_fonts = soup.find_all('font',{'color':'orange'})[1:]\n",
    "    for orange_font in orange_fonts:\n",
    "        #Second orange_font is the name\n",
    "        name = orange_font.get_text(strip=True)\n",
    "        #Subject is a hyperlink\n",
    "        subject = orange_font.find_next('a').get_text(strip=True)\n",
    "        # Text is enclosed within blockquote\n",
    "        text = orange_font.find_next('blockquote').get_text(strip=True)\n",
    "        spam_email_data.append({'Name':name, 'Subject':subject, 'Text': text})\n",
    "        count_spam_emails+=1\n",
    "        if count_spam_emails >= count_ham_emails:\n",
    "            break\n",
    "    if count_spam_emails >= count_ham_emails:\n",
    "            break\n",
    "# Convertion of scraped data into a DataFrame\n",
    "phish_emails_df= pd.DataFrame(spam_email_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bfa7e-1cd2-4465-a534-e571277dc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation of Subject and Content into a new Column text\n",
    "phish_emails_df['text'] =phish_emails_df['Subject']+phish_emails_df['Text']\n",
    "phish_emails_df['label'] = 1\n",
    "phish_emails_df = phish_emails_df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411f9b6-0a80-4ba3-b29c-137a21c6e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the number of spam email in the training dataset to be 24000 and the rest in testing.\n",
    "train_val = 24000\n",
    "total_len = phish_emails_df.shape[0]\n",
    "org_phish_train= phish_emails_df.sample(train_val)\n",
    "org_phish_test = phish_emails_df.drop(org_phish_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd8fc2-5337-4040-8ece-3ef58bf7be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emails.csv file contains the Enron dataset pre-downloaded from kaggle\n",
    "emails = pd.read_csv(\"emails.csv\")\n",
    "print(emails.head())\n",
    "# Print the number of emails \n",
    "print(\"The number of emails are:\")\n",
    "print(emails.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258ee21-b393-429f-b29c-a7fa5b678091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse dates safely\n",
    "def safe_parse_date(date_string):\n",
    "    if not date_string:\n",
    "        return None\n",
    "    try:\n",
    "        return parser.parse(date_string)\n",
    "    except (ValueError, parser.ParserError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30adbc4d-2b79-4e85-8e36-15096cfffc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract subject, body, date from each message\n",
    "df_email = pd.DataFrame([\n",
    "    {\n",
    "        'Content': complete_content.get_payload(),\n",
    "        'Subject': d.get('Subject', 'No Subject'),\n",
    "        'Date': d.get('Date'),  \n",
    "        'Label': 0\n",
    "    }\n",
    "    for i in range(emails.shape[0])\n",
    "    for email in [emails.loc[i]['message']]\n",
    "    for complete_content in [message_from_string(email)]\n",
    "    for d in [dict(complete_content.items())]\n",
    "])\n",
    "\n",
    "# Parse date column and extract year\n",
    "df_email['Date'] = pd.to_datetime(df_email['Date'].apply(safe_parse_date), errors='coerce')\n",
    "df_email['Year'] = df_email['Date'].dt.year\n",
    "\n",
    "# Combine Subject and Content into one text column\n",
    "df_email['text'] = df_email['Subject'] + df_email['Content']\n",
    "df_email['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc01cd-fba8-4b4c-811a-734d7febe506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for emails from January 1997 onwards till July 2001\n",
    "df_email['Date'] = pd.to_datetime(df_email['Date'])\n",
    "start_date = pd.Timestamp('1997-01-01', tz='UTC')  \n",
    "end_date = pd.Timestamp('2001-07-31 23:59:59', tz='UTC')\n",
    "filtered_emails = df_email[(df_email['Date'] >= start_date) & (df_email['Date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a932cb-fa4b-4b6a-a939-adfc34508865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 120K legitimate emails\n",
    "df_email = filtered_emails.sample(n=120000)\n",
    "phish_emails_df_final = df_email[['text', 'label']]\n",
    "num_of_train_emails = 96000\n",
    "\n",
    "# Split into train and test sets\n",
    "leg_train_df = phish_emails_df_final.head(num_of_train_emails)\n",
    "leg_test_df = phish_emails_df_final.drop(leg_train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae0da5-8810-470b-8824-ec2d0bab2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Train and Test Datasets (Dataset1)\n",
    "Dataset1_Train = pd.concat([org_phish_train,leg_train_df], axis = 0)\n",
    "Dataset1_Test = pd.concat([org_phish_test,leg_test_df], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec703b-46cc-486e-a972-c2de2b8df6ed",
   "metadata": {},
   "source": [
    "Tradional Preprocessing - The following fuctions are involved in the preprocessing stage\n",
    "1) Removal of special characters\n",
    "2) Conversion to lowercase\n",
    "3) Removal of stop words\n",
    "4) Removal of Numbers\n",
    "5) Identification of URL, Numbers, Phone, Email\n",
    "6) Detection and translation of languages (if not in English)\n",
    "These preprocessing steps are followed to obtain the Dataset1_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda8034-8c86-4c97-a4d9-d89f95b803e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove special characters from a given text string\n",
    "def remove_special_characters(text):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    cleaned_text = re.sub(r'_+', '', cleaned_text)\n",
    "    return cleaned_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8db06-800f-428b-b89a-0956c67e13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the text into lowercase\n",
    "def convert_to_lowercase(text):\n",
    "    words_list = [word.lower() for word in text.split()]\n",
    "    cleaned_text = ' '.join(words_list)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8b4a1-057e-48dd-8ee3-1218cbb635af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove stop words from text\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_list = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(words_list)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2745445-5c24-49de-b7c3-8f88dcd17746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove digits from the text\n",
    "def remove_numbers(text):\n",
    "    words = text.split()\n",
    "    cleaned_text_list = []\n",
    "    for word in words:\n",
    "        word_list = [x for x in word if x.isdigit()!=True]\n",
    "        cleaned_text_list.append(\"\".join(word_list))\n",
    "    cleaned_text = \" \".join(cleaned_text_list)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff7851-e161-4138-b35b-b0eb4702fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace URLs, emails, attachments, and phone numbers with keywords\n",
    "def url_attachment_identification(text):\n",
    "    url_pattern = r'\\b(?:https?:\\/\\/)?(?:[\\w-]+\\.)+[a-z]{2,}(?:\\/[^\\s]*)?'\n",
    "    attachment_pattern = r'\\b\\w+\\.(pdf|docx|jpg|png|xls|xlsx|ppt|pptx|txt|zip)\\b'\n",
    "    email_pattern = r'\\b[a-zA-Z0-9_.%+-]+@[a-zA-Z0-9.-]+\\.[a-z|A-Z]{2,7}\\b'\n",
    "    phone_pattern = r'^\\+?(\\d{1,3})?[-.\\s]?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}([-.\\s]?(ext|x|#)\\s?\\d{1,5})?$'\n",
    "\n",
    "    # Replace and count replacements for each pattern\n",
    "    cleaned_text = re.sub(email_pattern, \"email\", text)\n",
    "    cleaned_text = re.sub(phone_pattern, \"phone number\", cleaned_text)\n",
    "    cleaned_text = re.sub(url_pattern, \"link\", cleaned_text)\n",
    "    cleaned_text = re.sub(attachment_pattern, \"attachment\", cleaned_text)\n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17e1af-b722-4fb1-b462-56a00e98f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translates non-English text to English\n",
    "def detect_and_translate(text):\n",
    "    try:\n",
    "        detected_lang = langdetect.detect(text)\n",
    "        if detected_lang == 'en':\n",
    "            return text\n",
    "        translator = Translator()\n",
    "        translation = translator.translate(text, dest='en')\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951ed87-c7d5-4b6b-9084-4daf82711c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the emails\n",
    "def preprocess(text):\n",
    "    cleaned_text = url_attachment_identification(text)\n",
    "    cleaned_text = remove_special_characters(cleaned_text)\n",
    "    cleaned_text = convert_to_lowercase(cleaned_text)\n",
    "    cleaned_text = remove_stop_words(cleaned_text)\n",
    "    cleaned_text = remove_numbers(cleaned_text)\n",
    "    cleaned_text = detect_and_translate(cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389fa57-7674-4ac8-b618-d3491ccda4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the text column of dataframe df_email\n",
    "Dataset1_Train['cleaned_text'] = Dataset1_Train[\"text\"].apply(preprocess)\n",
    "Dataset1_Test['cleaned_text'] = Dataset1_Test[\"text\"].apply(preprocess)\n",
    "\n",
    "Dataset1_1Train = Dataset1_Train[[\"cleaned_text\",\"label\"]]\n",
    "Dataset1_1Test = Dataset1_Test[[\"cleaned_text\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a9224-9315-45e4-978e-47ad32cb93b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset1_1Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a38e3-fd73-47ae-9550-d22f8c8b3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset1_1Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3153cd-29a3-4396-8e7f-001dcfad3aec",
   "metadata": {},
   "source": [
    "Traditional + Split_Words + SpellChecker\n",
    "Along with the traditional methods used, we also use the second level of preprocessing functions - \n",
    "1) Splitting of Words\n",
    "2) Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46ccc6-2ef9-4518-941a-0d6d6335e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously saved Dataset1_1\n",
    "Dataset1_1Train = pd.read_csv(\"Dataset1_1Train_new3.csv\")\n",
    "Dataset1_1Test = pd.read_csv(\"Dataset1_1Test_new3.csv\")\n",
    "\n",
    "# Fill missing values\n",
    "Dataset1_1Train['cleaned_text'] = Dataset1_1Train['cleaned_text'].fillna('')\n",
    "Dataset1_1Test['cleaned_text'] = Dataset1_1Test['cleaned_text'].fillna('')\n",
    "\n",
    "# Remove index columns\n",
    "Dataset1_1Train.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "Dataset1_1Test.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63494631-1446-4558-a2ed-704b809efaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spelling correction\n",
    "def autocorrect_email(text):\n",
    "    check = Speller(lang=\"en\")\n",
    "    if isinstance(text, str):\n",
    "        autocorrected_email = check(text)\n",
    "        return autocorrected_email\n",
    "    else:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16846239-5838-4b14-8e6c-e4791b9573bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply autocorrect and word splitting on train and test datasets\n",
    "for index, row in tqdm(Dataset1_1Train.iterrows(), total=len(Dataset1_1Train), desc=\"Processing Training Data\"):\n",
    "    corrected = autocorrect_email(row['cleaned_text'])\n",
    "    Dataset1_1Train.loc[index, 'final_cleaned_text'] = \" \".join(wordninja.split(corrected))\n",
    "\n",
    "for index, row in tqdm(Dataset1_1Test.iterrows(), total=len(Dataset1_1Test), desc=\"Processing Testing Data\"):\n",
    "    corrected = autocorrect_email(row['cleaned_text'])\n",
    "    Dataset1_1Test.loc[index, 'final_cleaned_text'] = \" \".join(wordninja.split(corrected))\n",
    "\n",
    "# Final datasets\n",
    "Dataset1_2Train = Dataset1_1Train[[\"final_cleaned_text\", \"label\"]]\n",
    "Dataset1_2Test = Dataset1_1Test[[\"final_cleaned_text\", \"label\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab08e03-6b1d-4884-afeb-d57a0207b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to csv conversion\n",
    "#Dataset1_Train.to_csv(\"Dataset1_Train_new2.csv\")\n",
    "#Dataset1_Test.to_csv(\"Dataset1_Test_new2.csv\")\n",
    "#Dataset1_1Train.to_csv(\"Dataset1_1Train_new2.csv\")\n",
    "#Dataset1_1Test.to_csv(\"Dataset1_1Test_new2.csv\")\n",
    "#Dataset1_2Train.to_csv(\"Dataset1_2Train_new3.csv\")\n",
    "#ataset1_2Test.to_csv(\"Dataset1_2Test_new3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5aaa6-a638-4e84-aedd-017182da6475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
